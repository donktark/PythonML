{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donktark/Donktark.Python/blob/master/%EB%86%8D%EB%8B%B4%2C%EB%AA%85%EC%96%B8_%ED%85%8D%EC%8A%A4%ED%8A%B8_%EB%B6%84%EB%A5%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui7YCFUuxpg9"
      },
      "source": [
        "# 명언 vs 농담"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1pifnspz3CR",
        "outputId": "fefa6af3-bbd6-4903-c1c9-2416b984fcea"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install optuna\n",
        "!pip install datasets\n",
        "!pip install torch\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "wRObhntx9NWz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14835843-e6c0-41c9-a63a-0e4de70b3643"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.4.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.12.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ZV2dszbuxpg-"
      },
      "outputs": [],
      "source": [
        "#기본\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import requests\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "#전처리\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "#모델\n",
        "\n",
        "import torch\n",
        "import optuna\n",
        "from torch.utils.data import TensorDataset\n",
        "from datasets import Dataset\n",
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sYHpPnbExpg_"
      },
      "outputs": [],
      "source": [
        "#시드 고정\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "seed_everything(88)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNRnVu4KxphA"
      },
      "source": [
        "농담 : https://icanhazdadjoke.com/api <p>\n",
        "명언 : https://api.adviceslip.com/#object-search <p>\n",
        "　　　https://api.api-ninjas.com/v1/quotes?limit=10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WurSDoX1xphA"
      },
      "source": [
        "# 데이터 불러오기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AAElrQhxphA"
      },
      "source": [
        "### 농담 API를 데이터로 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "2ougyKlmxphA"
      },
      "outputs": [],
      "source": [
        "joke_list = []\n",
        "for page in list(range(38)) :\n",
        "    api_url = f'https://icanhazdadjoke.com/search?page={page}'\n",
        "    response = requests.get(api_url, headers = {'Accept': 'application/json'})\n",
        "    joke_json = response.json()\n",
        "    joke_list += ([i['joke'] for i in joke_json['results']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "u3yNP-2oxphA"
      },
      "outputs": [],
      "source": [
        "joke_df = pd.DataFrame({'text' : joke_list, 'category' : ['joke' for _ in list(range(len(joke_list)))]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "xTVIlLVnxphB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "ad5bccee-7158-40ea-f6d1-f723d7abc799"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  text category\n",
              "0    I'm tired of following my dreams. I'm just goi...     joke\n",
              "1    Did you hear about the guy whose whole left si...     joke\n",
              "2    Why didn’t the skeleton cross the road? Becaus...     joke\n",
              "3    What did one nut say as he chased another nut?...     joke\n",
              "4     Where do fish keep their money? In the riverbank     joke\n",
              "..                                                 ...      ...\n",
              "755  Why are giraffes so slow to apologize? Because...     joke\n",
              "756         \"Dad, I'm hungry.\" Hello, Hungry. I'm Dad.     joke\n",
              "757  I'm practicing for a bug-eating contest and I'...     joke\n",
              "758  I have the heart of a lion... and a lifetime b...     joke\n",
              "759  What do you call a guy lying on your doorstep?...     joke\n",
              "\n",
              "[760 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e62007bc-5f63-4257-8d01-353bec0a5262\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I'm tired of following my dreams. I'm just goi...</td>\n",
              "      <td>joke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Did you hear about the guy whose whole left si...</td>\n",
              "      <td>joke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Why didn’t the skeleton cross the road? Becaus...</td>\n",
              "      <td>joke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What did one nut say as he chased another nut?...</td>\n",
              "      <td>joke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Where do fish keep their money? In the riverbank</td>\n",
              "      <td>joke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>755</th>\n",
              "      <td>Why are giraffes so slow to apologize? Because...</td>\n",
              "      <td>joke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>756</th>\n",
              "      <td>\"Dad, I'm hungry.\" Hello, Hungry. I'm Dad.</td>\n",
              "      <td>joke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>757</th>\n",
              "      <td>I'm practicing for a bug-eating contest and I'...</td>\n",
              "      <td>joke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>758</th>\n",
              "      <td>I have the heart of a lion... and a lifetime b...</td>\n",
              "      <td>joke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>759</th>\n",
              "      <td>What do you call a guy lying on your doorstep?...</td>\n",
              "      <td>joke</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>760 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e62007bc-5f63-4257-8d01-353bec0a5262')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e62007bc-5f63-4257-8d01-353bec0a5262 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e62007bc-5f63-4257-8d01-353bec0a5262');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ff981110-c57e-412a-a7fd-7d5805055f4e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ff981110-c57e-412a-a7fd-7d5805055f4e')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ff981110-c57e-412a-a7fd-7d5805055f4e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "joke_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEvymITIxphB"
      },
      "source": [
        "### 명언 API를 데이터로 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "o6UZcrBtxphB"
      },
      "outputs": [],
      "source": [
        "quote_list = []\n",
        "\n",
        "#해당 API는 다수의 값을 요청할 수 없기 때문에 검색 기능에서 모음을 하나씩 넣고 합쳐서 중복 문구를 제거하는 것으로 함\n",
        "#1차에서 advice slip을 사용하였으나 데이터 불균형 문제로 api ninja에서 제공하는 api로 변경한 후 다시 학습.\n",
        "for _ in list(range(70)) :\n",
        "    api_url_q = 'https://api.api-ninjas.com/v1/quotes?limit=10'\n",
        "    response_q = requests.get(api_url_q, headers = {'X-Api-Key': 'lERfbKoPLYKszVzLyT5O0g==FnYn93fI3U19D6tB'})\n",
        "    quote_json = response_q.json()\n",
        "    quote_list += ([i['quote'] for i in quote_json])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "5nWE8WuCxphB"
      },
      "outputs": [],
      "source": [
        "quote_df = pd.DataFrame({'text' : quote_list, 'category' : ['advice' for _ in list(range(len(quote_list)))] }).drop_duplicates().reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "iaLnaaxpxphB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "908905f6-d8e0-494d-f815-4b2a816f0c00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  text category\n",
              "0    My friends and family are my support system. T...   advice\n",
              "1    A little sincerity is a dangerous thing, and a...   advice\n",
              "2    The thing that is really hard, and really amaz...   advice\n",
              "3    As a rule, software systems do not work well u...   advice\n",
              "4    Technology, like art, is a soaring exercise of...   advice\n",
              "..                                                 ...      ...\n",
              "755  Why are giraffes so slow to apologize? Because...     joke\n",
              "756         \"Dad, I'm hungry.\" Hello, Hungry. I'm Dad.     joke\n",
              "757  I'm practicing for a bug-eating contest and I'...     joke\n",
              "758  I have the heart of a lion... and a lifetime b...     joke\n",
              "759  What do you call a guy lying on your doorstep?...     joke\n",
              "\n",
              "[1457 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8f075d0d-32de-40f8-ae85-d3124adedeed\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>My friends and family are my support system. T...</td>\n",
              "      <td>advice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A little sincerity is a dangerous thing, and a...</td>\n",
              "      <td>advice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The thing that is really hard, and really amaz...</td>\n",
              "      <td>advice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>As a rule, software systems do not work well u...</td>\n",
              "      <td>advice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Technology, like art, is a soaring exercise of...</td>\n",
              "      <td>advice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>755</th>\n",
              "      <td>Why are giraffes so slow to apologize? Because...</td>\n",
              "      <td>joke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>756</th>\n",
              "      <td>\"Dad, I'm hungry.\" Hello, Hungry. I'm Dad.</td>\n",
              "      <td>joke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>757</th>\n",
              "      <td>I'm practicing for a bug-eating contest and I'...</td>\n",
              "      <td>joke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>758</th>\n",
              "      <td>I have the heart of a lion... and a lifetime b...</td>\n",
              "      <td>joke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>759</th>\n",
              "      <td>What do you call a guy lying on your doorstep?...</td>\n",
              "      <td>joke</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1457 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f075d0d-32de-40f8-ae85-d3124adedeed')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8f075d0d-32de-40f8-ae85-d3124adedeed button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8f075d0d-32de-40f8-ae85-d3124adedeed');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1b5dcd4d-d557-4230-86b8-6d6ca5a00575\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1b5dcd4d-d557-4230-86b8-6d6ca5a00575')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1b5dcd4d-d557-4230-86b8-6d6ca5a00575 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "full_df = pd.concat((quote_df, joke_df))\n",
        "full_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "4XsQZH98xphC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "9356ffde-0211-49ee-c871-c5e591837fe8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.hist(full_df['category'], bins=4, align='mid')\n",
        "for i in ['advice', 'joke']:\n",
        "    counts = len(full_df[full_df['category'] == i])\n",
        "    plt.text(i, counts-3, str(counts), ha = 'center', va = 'bottom')\n",
        "plt.rc('font', size = 12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCCfKQ_DxphC"
      },
      "source": [
        "# 2. 데이터 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewroumiIxphC"
      },
      "source": [
        "#### 특수문자 불용어 제거, 소문자 변환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "D31YOd5txphC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66cd0e86-78ad-4d4b-c7dc-d5d1cabc8b05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "A9fc5iPDxphC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "7a2fecd4-7b37-49bb-8813-49a3eaac459a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  text category\n",
              "0    friends family support system tell need hear w...   advice\n",
              "1    little sincerity dangerous thing great deal ab...   advice\n",
              "2    thing really hard really amazing giving perfec...   advice\n",
              "3    rule software systems work well used failed re...   advice\n",
              "4    technology like art soaring exercise human ima...   advice\n",
              "..                                                 ...      ...\n",
              "755  giraffes slow apologize takes long time swallo...     joke\n",
              "756                  dad im hungry hello hungry im dad     joke\n",
              "757  im practicing bugeating contest ive got butter...     joke\n",
              "758              heart lion lifetime ban san diego zoo     joke\n",
              "759                       call guy lying doorstep matt     joke\n",
              "\n",
              "[1457 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b49dc300-3505-48dd-8484-a49945a2102f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>friends family support system tell need hear w...</td>\n",
              "      <td>advice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>little sincerity dangerous thing great deal ab...</td>\n",
              "      <td>advice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>thing really hard really amazing giving perfec...</td>\n",
              "      <td>advice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>rule software systems work well used failed re...</td>\n",
              "      <td>advice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>technology like art soaring exercise human ima...</td>\n",
              "      <td>advice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>755</th>\n",
              "      <td>giraffes slow apologize takes long time swallo...</td>\n",
              "      <td>joke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>756</th>\n",
              "      <td>dad im hungry hello hungry im dad</td>\n",
              "      <td>joke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>757</th>\n",
              "      <td>im practicing bugeating contest ive got butter...</td>\n",
              "      <td>joke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>758</th>\n",
              "      <td>heart lion lifetime ban san diego zoo</td>\n",
              "      <td>joke</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>759</th>\n",
              "      <td>call guy lying doorstep matt</td>\n",
              "      <td>joke</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1457 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b49dc300-3505-48dd-8484-a49945a2102f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b49dc300-3505-48dd-8484-a49945a2102f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b49dc300-3505-48dd-8484-a49945a2102f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-58085c66-26fe-45fe-841a-d8cad0eaa2c7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-58085c66-26fe-45fe-841a-d8cad0eaa2c7')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-58085c66-26fe-45fe-841a-d8cad0eaa2c7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^\\w\\s\\d]', \"\", text)\n",
        "    text = text.lower()\n",
        "    text = text.split()\n",
        "    text = [w for w in text if w not in stop_words]\n",
        "    return \" \".join(text)\n",
        "\n",
        "full_df['text'] = full_df['text'].apply(lambda x: preprocess_text(x))\n",
        "full_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94cnpLmxxphC"
      },
      "source": [
        "#### 데이터 분할"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "OS_fXQ9txphD"
      },
      "outputs": [],
      "source": [
        "df_train, df_test = train_test_split(full_df, test_size=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgR-vjFYxphD"
      },
      "source": [
        "#### 텍스트 토큰화, 레이블 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "GlnxxioYxphD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "outputId": "06ca0c55-2327-4477-a734-4b3f3bfa0f9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([224., 431., 140.,  87.,  57.,  43.,  22.,  10.,   3.,   2.]),\n",
              " array([ 2. ,  5.6,  9.2, 12.8, 16.4, 20. , 23.6, 27.2, 30.8, 34.4, 38. ]),\n",
              " <BarContainer object of 10 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.hist([len(text.split()) for text in df_train['text']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylM-3GpDxphD"
      },
      "source": [
        "주로 15개 내외의 단어로 이루어진 문장이 많은 것으로 보인다. <p>\n",
        "데이터 변경 후에는 20대도 많으므로 max_length를 20으로 조정."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "CX3pvDyaxphD"
      },
      "outputs": [],
      "source": [
        "# 토큰화\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "def bt(text):\n",
        "    return bert_tokenizer(text, padding= 'max_length', max_length=15, truncation=True)\n",
        "\n",
        "train_bt = [bt(text) for text in df_train['text']]\n",
        "test_bt = [bt(text) for text in df_test['text']]\n",
        "\n",
        "# 목표변수 인코딩\n",
        "le = LabelEncoder()\n",
        "le.fit(df_train['category'])\n",
        "def making_labels(y):\n",
        "    return pd.DataFrame({'labels' : list(le.transform(y))})\n",
        "\n",
        "\n",
        "le_train = making_labels(df_train['category'])\n",
        "le_test = making_labels(df_test['category'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "kL7rJG26xphD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67b58373-c793-4d31-d203-443b0d2e17f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'advice': 0, 'joke': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "dict(zip(le.classes_, le.transform(le.classes_)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BIg-N_qxphD"
      },
      "source": [
        "#### 데이터셋으로 변환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "KuC5xWIrxphE"
      },
      "outputs": [],
      "source": [
        "ds_train = Dataset.from_pandas(pd.concat((pd.DataFrame(train_bt), le_train), axis=1), preserve_index=False)\n",
        "ds_test = Dataset.from_pandas(pd.concat((pd.DataFrame(test_bt), le_test), axis=1), preserve_index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "1nxwwxErxphE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc68ba19-f479-4854-9126-f9273750b8c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습용 데이터셋 : \n",
            " Dataset({\n",
            "    features: ['attention_mask', 'input_ids', 'token_type_ids', 'labels'],\n",
            "    num_rows: 1019\n",
            "})\n",
            "\n",
            " 검증용 데이터셋 : \n",
            " Dataset({\n",
            "    features: ['attention_mask', 'input_ids', 'token_type_ids', 'labels'],\n",
            "    num_rows: 438\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(\"학습용 데이터셋 : \\n\", ds_train)\n",
        "print(\"\\n 검증용 데이터셋 : \\n\", ds_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCOSAlBhxphE"
      },
      "source": [
        "# 3. 모델학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "mTN7BTluxphE"
      },
      "outputs": [],
      "source": [
        "# bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2)\n",
        "\n",
        "# #평가지표\n",
        "# def compute_metrix(pred):\n",
        "#     preds = pred.predictions.argmax(-1)\n",
        "#     return {\"accuracy\": accuracy_score(pred.label_ids, preds)}\n",
        "\n",
        "# #하이퍼 파라미터 목록\n",
        "# def optuna_hp_space(trial):\n",
        "#     return {\n",
        "#         'learning_rate' : trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True),\n",
        "#         \"per_device_train_batch_size\" : trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32, 64]),\n",
        "#         \"per_device_eval_batch_size\" : trial.suggest_categorical(\"per_device_eval_batch_size\", [8, 16, 32, 64])\n",
        "#     }\n",
        "\n",
        "\n",
        "# def model_trainer(trial):\n",
        "\n",
        "#     #튜닝할 하이퍼 파라미터\n",
        "\n",
        "\n",
        "#     #하이퍼 파라미터 설정\n",
        "#     training_args = TrainingArguments(\n",
        "#     output_dir=\"./bert_model\",\n",
        "#     learning_rate=lr,\n",
        "#     per_device_train_batch_size=pdtbs,\n",
        "#     per_device_eval_batch_size=pdebs,\n",
        "\n",
        "#     num_train_epochs=3,\n",
        "#     evaluation_strategy=\"steps\",\n",
        "#     save_total_limit=2,\n",
        "#     eval_steps=100,\n",
        "#     load_best_model_at_end=True,\n",
        "#     )\n",
        "\n",
        "#     #트레이너 설정\n",
        "#     trainer = Trainer(\n",
        "#         model = bert_model,\n",
        "#         args = training_args,\n",
        "#         train_dataset = ds_train,\n",
        "#         eval_dataset = ds_test,\n",
        "#         compute_metrics = compute_metrix\n",
        "#     )\n",
        "\n",
        "#     #모델 학습\n",
        "#     trainer.train()\n",
        "\n",
        "#     #평가지표 저장\n",
        "#     metrics = trainer.evaluate()\n",
        "#     return metrics[['accuracy']] #해석이 용이한 accuracy를 기준으로 학습\n",
        "\n",
        "# model_study = optuna.create_study(direction='maximize') #값을 최대화하는 방향으로 튜닝 (accuracy는 높아야하므로 maximize)\n",
        "# model_study.optimize(model_trainer, n_trials = 10)\n",
        "\n",
        "\n",
        "\n",
        "# best_model = model_study.best_params\n",
        "# best_score = model_study.best_value\n",
        "\n",
        "# print(\"최적 모델 : \\n\",best_model)\n",
        "# print(\"\\n 최적 스코어 : \\n\", best_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMhzhxwyxphE"
      },
      "source": [
        "위 코드에서 다음과 같은 오류가 발생했다. <p>\n",
        "TypeError: unhashable type: 'list'  <p>\n",
        "[W 2023-10-17 12:22:36,283] Trial 0 failed with value None.  <p>\n",
        "해결하진 못 했으나 다른 방법이 있기 때문에 하단의 다른 코드를 사용해 하이퍼파라미터 튜닝을 실시한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "OcBlLB5zxphE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b2b02d6d-4c63-4a5a-e859-35c65f5d37f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2023-10-21 12:32:03,679] A new study created in memory with name: no-name-962588eb-7c5d-41a9-9765-d1bb0ec6e5ab\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='768' max='768' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [768/768 01:31, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.830821</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.124254</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.760358</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.758281</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.804578</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.803235</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.764427</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.702671</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.757941</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.908500</td>\n",
              "      <td>0.822978</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.908500</td>\n",
              "      <td>0.815483</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.908500</td>\n",
              "      <td>0.699599</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.908500</td>\n",
              "      <td>0.721430</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.908500</td>\n",
              "      <td>0.734533</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.908500</td>\n",
              "      <td>0.695347</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-21 12:33:37,735] Trial 0 finished with value: 0.4287158746208291 and parameters: {'learning_rate': 0.0055540210835167405, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 32}. Best is trial 0 with value: 0.4287158746208291.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3060' max='3060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3060/3060 05:40, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.048864</td>\n",
              "      <td>0.431244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.806136</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.671054</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.695056</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.752460</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.694331</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.780496</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.759465</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.706076</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.734100</td>\n",
              "      <td>0.693290</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.734100</td>\n",
              "      <td>0.735715</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.734100</td>\n",
              "      <td>0.707169</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.734100</td>\n",
              "      <td>0.699347</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.734100</td>\n",
              "      <td>0.692980</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.734100</td>\n",
              "      <td>0.693787</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.734100</td>\n",
              "      <td>0.780774</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.734100</td>\n",
              "      <td>0.693297</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.734100</td>\n",
              "      <td>0.699277</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.734100</td>\n",
              "      <td>0.700140</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.708100</td>\n",
              "      <td>0.692881</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.708100</td>\n",
              "      <td>0.692735</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.708100</td>\n",
              "      <td>0.696779</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.708100</td>\n",
              "      <td>0.698670</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.708100</td>\n",
              "      <td>0.693804</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.708100</td>\n",
              "      <td>0.709652</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.708100</td>\n",
              "      <td>0.697605</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.708100</td>\n",
              "      <td>0.692738</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.708100</td>\n",
              "      <td>0.692733</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.708100</td>\n",
              "      <td>0.704493</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.701700</td>\n",
              "      <td>0.692708</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>0.701700</td>\n",
              "      <td>0.692986</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.701700</td>\n",
              "      <td>0.714210</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.701700</td>\n",
              "      <td>0.714134</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.701700</td>\n",
              "      <td>0.695300</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.701700</td>\n",
              "      <td>0.693010</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.701700</td>\n",
              "      <td>0.696121</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>0.701700</td>\n",
              "      <td>0.692639</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.701700</td>\n",
              "      <td>0.695266</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.701700</td>\n",
              "      <td>0.692901</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.696700</td>\n",
              "      <td>0.693711</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>0.696700</td>\n",
              "      <td>0.692981</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.696700</td>\n",
              "      <td>0.692754</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>0.696700</td>\n",
              "      <td>0.693110</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.696700</td>\n",
              "      <td>0.693285</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.696700</td>\n",
              "      <td>0.696221</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.696700</td>\n",
              "      <td>0.700912</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>0.696700</td>\n",
              "      <td>0.692776</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.696700</td>\n",
              "      <td>0.697641</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>0.696700</td>\n",
              "      <td>0.692637</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.697600</td>\n",
              "      <td>0.693084</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>0.697600</td>\n",
              "      <td>0.692638</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.697600</td>\n",
              "      <td>0.693792</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>0.697600</td>\n",
              "      <td>0.692640</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.697600</td>\n",
              "      <td>0.692764</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>0.697600</td>\n",
              "      <td>0.692671</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.697600</td>\n",
              "      <td>0.693414</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>0.697600</td>\n",
              "      <td>0.693148</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.697600</td>\n",
              "      <td>0.692679</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2950</td>\n",
              "      <td>0.697600</td>\n",
              "      <td>0.692730</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.692200</td>\n",
              "      <td>0.692669</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3050</td>\n",
              "      <td>0.692200</td>\n",
              "      <td>0.692639</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-21 12:39:20,250] Trial 1 finished with value: 0.5712841253791708 and parameters: {'learning_rate': 0.0001848144809533997, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 16}. Best is trial 1 with value: 0.5712841253791708.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='384' max='384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [384/384 00:50, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.698953</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.705079</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.825453</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.896234</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.725761</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.713562</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.697663</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-21 12:40:12,914] Trial 2 finished with value: 0.5712841253791708 and parameters: {'learning_rate': 0.005379191927575034, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 16}. Best is trial 1 with value: 0.5712841253791708.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='768' max='768' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [768/768 01:43, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.684093</td>\n",
              "      <td>0.622346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.703874</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.694635</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.704675</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.827000</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.692754</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.753205</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.697851</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.717830</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.730300</td>\n",
              "      <td>0.751334</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.730300</td>\n",
              "      <td>0.692939</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.730300</td>\n",
              "      <td>0.704718</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.730300</td>\n",
              "      <td>0.693628</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.730300</td>\n",
              "      <td>0.693155</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.730300</td>\n",
              "      <td>0.694867</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-21 12:41:57,779] Trial 3 finished with value: 0.4287158746208291 and parameters: {'learning_rate': 0.0011678113846351143, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 4}. Best is trial 1 with value: 0.5712841253791708.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3060' max='3060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3060/3060 06:38, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.366172</td>\n",
              "      <td>0.876138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.575059</td>\n",
              "      <td>0.725986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.510378</td>\n",
              "      <td>0.805865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.636796</td>\n",
              "      <td>0.626390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.411828</td>\n",
              "      <td>0.861982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.521273</td>\n",
              "      <td>0.899393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.502580</td>\n",
              "      <td>0.899393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.398720</td>\n",
              "      <td>0.886249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.466847</td>\n",
              "      <td>0.895349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.533700</td>\n",
              "      <td>0.349442</td>\n",
              "      <td>0.895854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.533700</td>\n",
              "      <td>0.492002</td>\n",
              "      <td>0.886754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.533700</td>\n",
              "      <td>0.404956</td>\n",
              "      <td>0.888777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.533700</td>\n",
              "      <td>0.688983</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.533700</td>\n",
              "      <td>0.389702</td>\n",
              "      <td>0.860971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.533700</td>\n",
              "      <td>0.431965</td>\n",
              "      <td>0.882204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.533700</td>\n",
              "      <td>0.628026</td>\n",
              "      <td>0.880182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.533700</td>\n",
              "      <td>0.576311</td>\n",
              "      <td>0.874115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.533700</td>\n",
              "      <td>0.515620</td>\n",
              "      <td>0.893832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.533700</td>\n",
              "      <td>0.373803</td>\n",
              "      <td>0.915066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.500600</td>\n",
              "      <td>0.332351</td>\n",
              "      <td>0.907482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.500600</td>\n",
              "      <td>0.430841</td>\n",
              "      <td>0.908493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.500600</td>\n",
              "      <td>0.631657</td>\n",
              "      <td>0.855410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.500600</td>\n",
              "      <td>0.343395</td>\n",
              "      <td>0.907482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.500600</td>\n",
              "      <td>0.437861</td>\n",
              "      <td>0.903438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.500600</td>\n",
              "      <td>0.491419</td>\n",
              "      <td>0.898382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.500600</td>\n",
              "      <td>0.674530</td>\n",
              "      <td>0.879171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.500600</td>\n",
              "      <td>1.136660</td>\n",
              "      <td>0.589484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.500600</td>\n",
              "      <td>0.743943</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.500600</td>\n",
              "      <td>0.709859</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.543300</td>\n",
              "      <td>0.696111</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>0.543300</td>\n",
              "      <td>0.699966</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.543300</td>\n",
              "      <td>0.723410</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.543300</td>\n",
              "      <td>0.713000</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.543300</td>\n",
              "      <td>0.694381</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.543300</td>\n",
              "      <td>0.693673</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.543300</td>\n",
              "      <td>0.695442</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>0.543300</td>\n",
              "      <td>0.695441</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.543300</td>\n",
              "      <td>0.692824</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.543300</td>\n",
              "      <td>0.694195</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.704300</td>\n",
              "      <td>0.693281</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>0.704300</td>\n",
              "      <td>0.692637</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.704300</td>\n",
              "      <td>0.693224</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>0.704300</td>\n",
              "      <td>0.694239</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.704300</td>\n",
              "      <td>0.693101</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.704300</td>\n",
              "      <td>0.697269</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.704300</td>\n",
              "      <td>0.702469</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>0.704300</td>\n",
              "      <td>0.693935</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.704300</td>\n",
              "      <td>0.695005</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>0.704300</td>\n",
              "      <td>0.692972</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.700400</td>\n",
              "      <td>0.692638</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>0.700400</td>\n",
              "      <td>0.692884</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.700400</td>\n",
              "      <td>0.694993</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>0.700400</td>\n",
              "      <td>0.692691</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.700400</td>\n",
              "      <td>0.694020</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>0.700400</td>\n",
              "      <td>0.692806</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.700400</td>\n",
              "      <td>0.695053</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>0.700400</td>\n",
              "      <td>0.693345</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.700400</td>\n",
              "      <td>0.692682</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2950</td>\n",
              "      <td>0.700400</td>\n",
              "      <td>0.692879</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.695500</td>\n",
              "      <td>0.692673</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3050</td>\n",
              "      <td>0.695500</td>\n",
              "      <td>0.692642</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-21 12:48:38,567] Trial 4 finished with value: 0.5712841253791708 and parameters: {'learning_rate': 0.00011749586618500617, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 4}. Best is trial 1 with value: 0.5712841253791708.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='384' max='384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [384/384 00:58, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.695548</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.701630</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.758294</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.819368</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.694638</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.696185</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.704104</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-21 12:49:40,017] Trial 5 finished with value: 0.5712841253791708 and parameters: {'learning_rate': 0.0064469018346047045, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 4}. Best is trial 1 with value: 0.5712841253791708.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='1536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  50/1536 00:04 < 02:30, 9.86 it/s, Epoch 0/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.708141</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-21 12:49:46,396] Trial 6 pruned. \n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 50/192 00:12 < 00:35, 3.99 it/s, Epoch 3/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.721136</td>\n",
              "      <td>0.428716</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-21 12:50:00,639] Trial 7 pruned. \n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [192/192 00:41, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.701749</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.692683</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.699387</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-21 12:50:44,908] Trial 8 finished with value: 0.5712841253791708 and parameters: {'learning_rate': 0.001521782660239175, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 8}. Best is trial 1 with value: 0.5712841253791708.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='384' max='384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [384/384 00:50, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.243410</td>\n",
              "      <td>0.914560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.493272</td>\n",
              "      <td>0.910010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.421518</td>\n",
              "      <td>0.932760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.550178</td>\n",
              "      <td>0.919110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.525975</td>\n",
              "      <td>0.923660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.536387</td>\n",
              "      <td>0.921638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.542582</td>\n",
              "      <td>0.921638</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-21 12:51:37,307] Trial 9 finished with value: 0.9216380182002022 and parameters: {'learning_rate': 4.293031986019314e-05, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32}. Best is trial 9 with value: 0.9216380182002022.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='384' max='384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [384/384 00:50, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.258067</td>\n",
              "      <td>0.898888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.293678</td>\n",
              "      <td>0.911527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.411936</td>\n",
              "      <td>0.919616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.478604</td>\n",
              "      <td>0.924671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.482078</td>\n",
              "      <td>0.932760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.493614</td>\n",
              "      <td>0.927705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.495828</td>\n",
              "      <td>0.927705</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-21 12:52:29,591] Trial 10 finished with value: 0.9277047522750252 and parameters: {'learning_rate': 2.006496920169984e-05, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32}. Best is trial 10 with value: 0.9277047522750252.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='384' max='384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [384/384 00:50, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.279587</td>\n",
              "      <td>0.893832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.279338</td>\n",
              "      <td>0.913549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.360951</td>\n",
              "      <td>0.931244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.373648</td>\n",
              "      <td>0.912538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.430251</td>\n",
              "      <td>0.915066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.460556</td>\n",
              "      <td>0.917594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.470913</td>\n",
              "      <td>0.912538</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-21 12:53:21,534] Trial 11 finished with value: 0.9125379170879676 and parameters: {'learning_rate': 1.747106344473912e-05, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32}. Best is trial 10 with value: 0.9277047522750252.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='384' max='384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [384/384 00:50, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.333077</td>\n",
              "      <td>0.895854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.248026</td>\n",
              "      <td>0.906977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.317493</td>\n",
              "      <td>0.906977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.361984</td>\n",
              "      <td>0.915571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.393549</td>\n",
              "      <td>0.901416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.416611</td>\n",
              "      <td>0.914560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.431696</td>\n",
              "      <td>0.912538</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-21 12:54:13,447] Trial 12 finished with value: 0.9125379170879676 and parameters: {'learning_rate': 1.2816322922646813e-05, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32}. Best is trial 10 with value: 0.9277047522750252.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='384' max='384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [384/384 00:50, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.248010</td>\n",
              "      <td>0.923155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.451753</td>\n",
              "      <td>0.911527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.427756</td>\n",
              "      <td>0.926188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.498389</td>\n",
              "      <td>0.925683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.513428</td>\n",
              "      <td>0.914560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.512569</td>\n",
              "      <td>0.916077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.522491</td>\n",
              "      <td>0.916077</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-21 12:55:06,338] Trial 13 finished with value: 0.916076845298281 and parameters: {'learning_rate': 4.085198549140344e-05, 'per_device_train_batch_size': 32, 'per_device_eval_batch_size': 32}. Best is trial 10 with value: 0.9277047522750252.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1536' max='1536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1536/1536 02:42, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.306340</td>\n",
              "      <td>0.889788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.229661</td>\n",
              "      <td>0.927705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.237094</td>\n",
              "      <td>0.922144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.395498</td>\n",
              "      <td>0.919110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.403474</td>\n",
              "      <td>0.913549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.346033</td>\n",
              "      <td>0.924166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.336286</td>\n",
              "      <td>0.939838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.513670</td>\n",
              "      <td>0.915066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.431931</td>\n",
              "      <td>0.925683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.188200</td>\n",
              "      <td>0.462948</td>\n",
              "      <td>0.926188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.188200</td>\n",
              "      <td>0.504185</td>\n",
              "      <td>0.928210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.188200</td>\n",
              "      <td>0.560062</td>\n",
              "      <td>0.922144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.188200</td>\n",
              "      <td>0.417236</td>\n",
              "      <td>0.938827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.188200</td>\n",
              "      <td>0.525497</td>\n",
              "      <td>0.927199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.188200</td>\n",
              "      <td>0.515640</td>\n",
              "      <td>0.921638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.188200</td>\n",
              "      <td>0.531204</td>\n",
              "      <td>0.921638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.188200</td>\n",
              "      <td>0.539977</td>\n",
              "      <td>0.926694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.188200</td>\n",
              "      <td>0.548549</td>\n",
              "      <td>0.924671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.188200</td>\n",
              "      <td>0.550085</td>\n",
              "      <td>0.927199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.004400</td>\n",
              "      <td>0.562566</td>\n",
              "      <td>0.924671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.004400</td>\n",
              "      <td>0.569370</td>\n",
              "      <td>0.924671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.004400</td>\n",
              "      <td>0.575768</td>\n",
              "      <td>0.924671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.004400</td>\n",
              "      <td>0.579833</td>\n",
              "      <td>0.924671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.004400</td>\n",
              "      <td>0.583512</td>\n",
              "      <td>0.924671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.004400</td>\n",
              "      <td>0.549842</td>\n",
              "      <td>0.941355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.004400</td>\n",
              "      <td>0.538004</td>\n",
              "      <td>0.936299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.004400</td>\n",
              "      <td>0.557741</td>\n",
              "      <td>0.932255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.004400</td>\n",
              "      <td>0.558878</td>\n",
              "      <td>0.932255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.004400</td>\n",
              "      <td>0.618954</td>\n",
              "      <td>0.924166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.001600</td>\n",
              "      <td>0.620669</td>\n",
              "      <td>0.924166</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-21 12:57:50,608] Trial 14 finished with value: 0.9241658240647118 and parameters: {'learning_rate': 4.7342389637695096e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 32}. Best is trial 10 with value: 0.9277047522750252.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1536' max='1536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1536/1536 02:46, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.283644</td>\n",
              "      <td>0.908493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.242298</td>\n",
              "      <td>0.914055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.375756</td>\n",
              "      <td>0.899393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.445355</td>\n",
              "      <td>0.904954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.324761</td>\n",
              "      <td>0.920121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.495636</td>\n",
              "      <td>0.917088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.461160</td>\n",
              "      <td>0.924166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.446876</td>\n",
              "      <td>0.928210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.539168</td>\n",
              "      <td>0.917088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.195800</td>\n",
              "      <td>0.511992</td>\n",
              "      <td>0.927705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.195800</td>\n",
              "      <td>0.571009</td>\n",
              "      <td>0.920627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.195800</td>\n",
              "      <td>0.568045</td>\n",
              "      <td>0.930233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.195800</td>\n",
              "      <td>0.602862</td>\n",
              "      <td>0.920627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.195800</td>\n",
              "      <td>0.604813</td>\n",
              "      <td>0.920627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.195800</td>\n",
              "      <td>0.574871</td>\n",
              "      <td>0.930738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.195800</td>\n",
              "      <td>0.588845</td>\n",
              "      <td>0.930738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.195800</td>\n",
              "      <td>0.620738</td>\n",
              "      <td>0.925177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.195800</td>\n",
              "      <td>0.629403</td>\n",
              "      <td>0.925177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.195800</td>\n",
              "      <td>0.639807</td>\n",
              "      <td>0.923155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.003500</td>\n",
              "      <td>0.652274</td>\n",
              "      <td>0.923155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.003500</td>\n",
              "      <td>0.656211</td>\n",
              "      <td>0.923155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.003500</td>\n",
              "      <td>0.640478</td>\n",
              "      <td>0.925683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.003500</td>\n",
              "      <td>0.635445</td>\n",
              "      <td>0.927199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.003500</td>\n",
              "      <td>0.625472</td>\n",
              "      <td>0.931244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.003500</td>\n",
              "      <td>0.627940</td>\n",
              "      <td>0.931244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.003500</td>\n",
              "      <td>0.605976</td>\n",
              "      <td>0.931244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.003500</td>\n",
              "      <td>0.608987</td>\n",
              "      <td>0.931244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.003500</td>\n",
              "      <td>0.610938</td>\n",
              "      <td>0.931244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.003500</td>\n",
              "      <td>0.612175</td>\n",
              "      <td>0.931244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.002300</td>\n",
              "      <td>0.612693</td>\n",
              "      <td>0.931244</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-21 13:00:38,208] Trial 15 finished with value: 0.9312436804853387 and parameters: {'learning_rate': 4.644981822789779e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 64}. Best is trial 15 with value: 0.9312436804853387.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='1536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  50/1536 00:04 < 02:09, 11.52 it/s, Epoch 0/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.528043</td>\n",
              "      <td>0.882710</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-21 13:00:43,982] Trial 16 pruned. \n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='1536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  50/1536 00:04 < 02:20, 10.58 it/s, Epoch 0/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.692904</td>\n",
              "      <td>0.571284</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-21 13:00:50,041] Trial 17 pruned. \n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1536' max='1536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1536/1536 02:47, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.317561</td>\n",
              "      <td>0.889282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.279572</td>\n",
              "      <td>0.911021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.373326</td>\n",
              "      <td>0.915066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.327738</td>\n",
              "      <td>0.929221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.386335</td>\n",
              "      <td>0.921638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.534233</td>\n",
              "      <td>0.895854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.443846</td>\n",
              "      <td>0.910516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.439962</td>\n",
              "      <td>0.912538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.345010</td>\n",
              "      <td>0.929727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.268200</td>\n",
              "      <td>0.320165</td>\n",
              "      <td>0.922144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.268200</td>\n",
              "      <td>0.382282</td>\n",
              "      <td>0.938322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.268200</td>\n",
              "      <td>0.420240</td>\n",
              "      <td>0.917594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.268200</td>\n",
              "      <td>0.439813</td>\n",
              "      <td>0.927199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.268200</td>\n",
              "      <td>0.371596</td>\n",
              "      <td>0.944894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.268200</td>\n",
              "      <td>0.367793</td>\n",
              "      <td>0.945905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.268200</td>\n",
              "      <td>0.415223</td>\n",
              "      <td>0.937816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.268200</td>\n",
              "      <td>0.534330</td>\n",
              "      <td>0.913043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.268200</td>\n",
              "      <td>0.480594</td>\n",
              "      <td>0.943883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.268200</td>\n",
              "      <td>0.480875</td>\n",
              "      <td>0.939838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.034100</td>\n",
              "      <td>0.517294</td>\n",
              "      <td>0.934277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.034100</td>\n",
              "      <td>0.523534</td>\n",
              "      <td>0.934783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.034100</td>\n",
              "      <td>0.514623</td>\n",
              "      <td>0.937310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.034100</td>\n",
              "      <td>0.519903</td>\n",
              "      <td>0.937310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.034100</td>\n",
              "      <td>0.524000</td>\n",
              "      <td>0.937310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.034100</td>\n",
              "      <td>0.528597</td>\n",
              "      <td>0.937310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.034100</td>\n",
              "      <td>0.531752</td>\n",
              "      <td>0.937310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.034100</td>\n",
              "      <td>0.534545</td>\n",
              "      <td>0.937310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.034100</td>\n",
              "      <td>0.536384</td>\n",
              "      <td>0.937310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.034100</td>\n",
              "      <td>0.527120</td>\n",
              "      <td>0.939838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.002100</td>\n",
              "      <td>0.525902</td>\n",
              "      <td>0.939838</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-21 13:03:39,481] Trial 18 finished with value: 0.9398382204246714 and parameters: {'learning_rate': 7.361580217705999e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 64}. Best is trial 18 with value: 0.9398382204246714.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='1536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  50/1536 00:04 < 02:25, 10.21 it/s, Epoch 0/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.336180</td>\n",
              "      <td>0.876138</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-21 13:03:45,748] Trial 19 pruned. \n"
          ]
        }
      ],
      "source": [
        "# bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2)\n",
        "\n",
        "def model_init(trial):\n",
        "    return BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2)\n",
        "\n",
        "\n",
        "#평가지표\n",
        "def compute_metrix(pred):\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    labels = pred.label_ids\n",
        "    class_weights = [1.0, 1.25] # 0: advice, 1: joke\n",
        "    weight_acc = accuracy_score(labels, preds, sample_weight = [class_weights[label] for label in labels])\n",
        "    return {\"accuracy\": weight_acc}\n",
        "\n",
        "#하이퍼 파라미터 목록\n",
        "def optuna_hp_space(trial):\n",
        "    return {\n",
        "        'learning_rate' : trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True),\n",
        "        \"per_device_train_batch_size\" : trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8, 16, 32, 64]),\n",
        "        \"per_device_eval_batch_size\" : trial.suggest_categorical(\"per_device_eval_batch_size\", [4, 8, 16, 32, 64])\n",
        "    }\n",
        "\n",
        "\n",
        "#하이퍼 파라미터 입력\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert_model\",\n",
        "    # learning_rate=learning_rate,\n",
        "    # per_device_train_batch_size=per_device_train_batch_size,\n",
        "    # per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "\n",
        "    num_train_epochs=12,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_total_limit=2,\n",
        "    eval_steps=50,\n",
        "    load_best_model_at_end=True, #데이터 변경 후 변경함. 후에 다시 할때는 True로 바꿀것\n",
        ")\n",
        "\n",
        "#트레이너 설정\n",
        "trainer = Trainer(\n",
        "    model = None,\n",
        "    model_init = model_init,\n",
        "    args = training_args,\n",
        "    train_dataset = ds_train,\n",
        "    eval_dataset = ds_test,\n",
        "    compute_metrics = compute_metrix\n",
        ")\n",
        "\n",
        "best_trial = trainer.hyperparameter_search(\n",
        "    direction = \"maximize\",\n",
        "    backend = 'optuna',\n",
        "    hp_space = optuna_hp_space,\n",
        "    n_trials = 20,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "L938EktrxphF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaa9de27-ea5e-45e1-c7df-1e389f059d38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BestRun(run_id='18', objective=0.9398382204246714, hyperparameters={'learning_rate': 7.361580217705999e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 64}, run_summary=None)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "best_trial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLapA-Q8xphF"
      },
      "source": [
        "1차 : 정확도가 약 0.929로 높게 나타났다. 이 모델을 이용해 다른 농담과 명언이 주어졌을 때 어떻게 분류할 지 확인할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gux5SbS6xphF"
      },
      "source": [
        "# 4. 새로운 데이터 예측"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "ykTpWg1MxphF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de3802d-840e-4554-9a54-d1e195e0f3ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "best_hp = best_trial.hyperparameters\n",
        "best_model = model_init(best_hp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ltjKWrixphF"
      },
      "source": [
        "새로운 api에서 유사한 데이터를 받아 동일한 전처리를 한 후 모델에 적용시킨다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "qr3461-qxphG"
      },
      "outputs": [],
      "source": [
        "#농담\n",
        "new_api_url_j = 'https://official-joke-api.appspot.com/random_ten'\n",
        "new_response_j = requests.get(new_api_url_j)\n",
        "new_joke_json = new_response_j.json()\n",
        "joke_new = [i['setup'] + \" \" + i['punchline'] for i in new_joke_json]\n",
        "joke_new_df = pd.DataFrame({'text' : joke_new, 'category' : ['joke' for _ in list(range(len(joke_new)))]})\n",
        "\n",
        "#명언\n",
        "num = random.randint(1,38)\n",
        "new_api_url_q = 'https://quote-garden.onrender.com/api/v3/quotes/?page=' + f'{num}'\n",
        "new_response_q = requests.get(new_api_url_q)\n",
        "new_quote_json = new_response_q.json()\n",
        "quote_new = [i['quoteText'] for i in new_quote_json['data']]\n",
        "quote_new_df = pd.DataFrame({'text' : quote_new, 'category' : ['advice' for _ in list(range(len(quote_new)))]})\n",
        "\n",
        "new_df = pd.concat((joke_new_df, quote_new_df), axis = 0)\n",
        "\n",
        "new_df['text'] = new_df['text'].apply(lambda x: preprocess_text(x))\n",
        "\n",
        "new_bt = [bt(text) for text in new_df['text']]\n",
        "new_le = making_labels(new_df['category'])\n",
        "\n",
        "ds_new = Dataset.from_pandas(pd.concat((pd.DataFrame(new_bt), new_le), axis = 1), preserve_index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "RdLvwIAVxphG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "980489f1-25ba-4688-de72-b24ab1cf7ceb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ],
      "source": [
        "new_pred = Trainer(model = best_model).predict(ds_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "H4DNxEjXxphN"
      },
      "outputs": [],
      "source": [
        "new_logits = torch.tensor(new_pred.predictions)\n",
        "new_preds = new_logits.argmax(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "bmGukOn2xphN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59957238-cee8-4b82-8ec6-39de0293943e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예측 :  tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "예측 정확도 :  0.65\n"
          ]
        }
      ],
      "source": [
        "print('예측 : ', new_preds)\n",
        "print('예측 정확도 : ', accuracy_score(new_preds, ds_new['labels']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2SeiHR5xphN"
      },
      "source": [
        "1차 : tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) <p>\n",
        "주로 joke로 많이 예측하는 것을 알 수 있다. 이는 학습된 데이터가 joke가 더 많은 불균형이 있어 advice에 그만큼 가중치를 부여할 필요가 있다고 판단되었다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrlZr-xsxphO"
      },
      "source": [
        "2차 : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) <p>\n",
        "데이터 추가 후에는 오히려 모두 advice로 분류하고 있다."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}